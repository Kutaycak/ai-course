{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "import collections\n",
    "\n",
    "sc=SparkContext()\n",
    "rdd=sc.parallelize([3,4,56,7.4,2])\n",
    "\n",
    "sq=rdd.map(lambda x: x*x)\n",
    "print(sq.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('RatingsHistogram')\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile('u.data')\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('MinimumTemperature')\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile('1800.csv')\n",
    "parsedLines = lines.map(parseLine)\n",
    "minTemps = parsedLines.filter(lambda x: 'TMIN' in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('MaxTemperature')\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile('1800.csv')\n",
    "parsedLines = lines.map(parseLine)\n",
    "maxTemps = parsedLines.filter(lambda x: 'TMAX' in x[1])\n",
    "stationTemps = maxTemps.map(lambda x: (x[0], x[2]))\n",
    "maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
    "\n",
    "results = maxTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n",
    "\n",
    "sc.stop()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('WordCount')\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "input = sc.textFile('book.txt')\n",
    "words = input.flatMap(normalizeWords)\n",
    "\n",
    "wordCounts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "wordCountsSorted = wordCounts.map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "results = wordCountsSorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if (word):\n",
    "        print(word.decode() + \":\\t\\t\" + count)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('PopularHero')\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "def countCoOccurences(line):\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) - 1)\n",
    "\n",
    "def parseNames(line):\n",
    "    fields = line.split('\\\"')\n",
    "    return (int(fields[0]), fields[1].encode(\"utf8\"))\n",
    "\n",
    "names = sc.textFile(\"Marvel-names.txt\")\n",
    "namesRdd = names.map(parseNames)\n",
    "\n",
    "lines = sc.textFile(\"Marvel-graph.txt\")\n",
    "\n",
    "pairings = lines.map(countCoOccurences)\n",
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n",
    "flipped = totalFriendsByCharacter.map(lambda x : (x[1], x[0]))\n",
    "\n",
    "mostPopular = flipped.max()\n",
    "\n",
    "mostPopularName = namesRdd.lookup(mostPopular[1])[0]\n",
    "\n",
    "print(str(mostPopularName) + \" is the most popular superhero, with \" + str(mostPopular[0]) + \" co-appearances.\")\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Pima Indian Classifier\").getOrCreate()\n",
    "\n",
    "data=spark.read.csv(\"pima-indians-diabetes.csv\",header=True,inferSchema=True)\n",
    "\n",
    "for col in data.columns[:-1]:\n",
    "    if col !='Outcome':\n",
    "        data=data.withColumn(col, data[col].cast(IntegerType())) \n",
    "\n",
    "predictors=list(data.columns[:-1])\n",
    "assembler=VectorAssembler(inputCols=predictors, outputCol=\"features\")\n",
    "data=assembler.transform(data).select('features', 'Outcome')\n",
    "\n",
    "train,test=data.randomSplit([0.8, 0.2], seed=42)\n",
    "lr=LogisticRegression(labelCol=\"Outcome\",featuresCol=\"features\")\n",
    "model=lr.fit(train)\n",
    "predictions=model.transform(test)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator(labelCol=\"Outcome\")\n",
    "accuracy=evaluator.evaluate(predictions)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
